{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daeddcd7",
   "metadata": {},
   "source": [
    "Using the average gold prices for trying to model the gold prices with the different variables. when we test for linearity, using the multiple scatterplot of the dependent variable against the independent variables, it is possible to see a linearity with almost all variables, but the MSCI's variables do not seem  to have as clear of a linearity compared with the others. By Using a scatterplot with the residuals against the dependent variable, we can see that the residuals don't seem to have any especific pattern. The residuals vs the independent values appear to be independent. Using a qqplot we can see that the residuals follow the red line and the normality tests(shapiro-wilk and kolmogorov) show that the standardized residuals are normally distributed, we don't reject the null hypothesis for the shapiro-wilk, so for alpha equal to 0.05 the standardized residuals are normally distributed. The kolmogorov we dont reject the null hypothesis with p-value equal to 0.7506071234246195 so for alpha equal to 0.05 the 2 sampindependent les are drawn from the same continuous distribution. the Anderson-Darling test also indicate a normality in the residuals.\n",
    "\n",
    "    (The observations you apply your tests to (some form of residuals) aren't independent, so the usual statistics don't have the correct distribution. Further, strictly speaking, none of the residuals you consider will be exactly normal, since your data will never be exactly normal. )\n",
    "    \n",
    "    (Even if your data were to be exactly normal, neither the studentized residuals nor the standardized residuals would be exactly normal. Nevertheless it's much more common for people to examine those (say by QQ plots) than the raw residuals.)\n",
    "\n",
    "Linearity:\n",
    "    \n",
    "    Linearity. This means that the mean of the response variable is a linear combination of the parameters (regression coefficients) and the predictor variables. Note that this assumption is much less restrictive than it may at first seem.\n",
    "\n",
    "\n",
    "Homoscedasicity assumptions:\n",
    "\n",
    "    The last assumption of multiple linear regression is homoscedasticity. A scatterplot of residuals versus predicted values is good way to check for homoscedasticity. There should be no clear pattern in the distribution; if there is a cone-shaped pattern (as shown below), the data is heteroscedastic.\n",
    "    \n",
    "    The sixth assumption of linear regression is homoscedasticity. Homoscedasticity in a model means that the error is constant along the values of the dependent variable. The best way for checking homoscedasticity is to make a scatterplot with the residuals against the dependent variable.\n",
    "    \n",
    "Independence: \n",
    "\n",
    "    To test for non-time-series violations of independence, you can look at plots of the residuals versus independent variables or plots of residuals versus row number in situations where the rows have been sorted or grouped in some way that depends (only) on the values of the independent variables.\n",
    "    \n",
    "    \n",
    "Normality:\n",
    "\n",
    "    If the theoretical residuals are not exactly normally distributed, but the sample size is large enough then the Central Limit Theorem says that the usual inference (tests and confidence intervals, but not necessarily prediction intervals) based on the assumption of normality will still be approximately correct.\n",
    "    Also note that the tests of normality are rule out tests, they can tell you that the data is unlikely to have come from a normal distribution. But if the test is not significant that does not mean that the data came from a normal distribution, it could also mean that you just don't have enough power to see the difference. Larger sample sizes give more power to detect the non-normality, but larger samples and the CLT mean that the non-normality is least important. So for small sample sizes the assumption of normality is important but the tests are meaningless, for large sample sizes the tests may be more accurate, but the question of exact normality becomes meaningless.\n",
    "     Statistical theory says its okay just to assume that  and . Once you do that, determining the percentiles of the standard normal curve is straightforward. The p-th percentile value reduces to just a \"Z-score\" (or \"normal score\"). \n",
    "    \n",
    "Shapiro-Wilk test:\n",
    "\n",
    "    The Shapiro–Wilk test tests the null hypothesis that a sample x1, ..., xn came from a normally distributed population. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.\n",
    "    \n",
    "Kolmogorov-smirnov:\n",
    "    \n",
    "    This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution. In practice, the statistic requires a relatively large number of data points (in comparison to other goodness of fit criteria such as the Anderson–Darling test statistic) to properly reject the null hypothesis. \n",
    "    \n",
    "    two-sided: The null hypothesis is that the two distributions are identical, F(x)=G(x) for all x; the alternative is that they are not identical.\n",
    "\n",
    "    less: The null hypothesis is that F(x) >= G(x) for all x; the alternative is that F(x) < G(x) for at least one x.\n",
    "\n",
    "    greater: The null hypothesis is that F(x) <= G(x) for all x; the alternative is that F(x) > G(x) for at least one x.\n",
    "    \n",
    "    \n",
    "    Critical values provided are for the following significance levels:\n",
    "\n",
    "Anderson-Darling test:\n",
    "\n",
    "    The Anderson-Darling test tests the null hypothesis that a sample is drawn from a population that follows a particular distribution.\n",
    "    \n",
    "    normal/exponential\n",
    "    15%, 10%, 5%, 2.5%, 1%\n",
    "\n",
    "    logistic\n",
    "    25%, 10%, 5%, 2.5%, 1%, 0.5%\n",
    "\n",
    "    Gumbel\n",
    "    25%, 10%, 5%, 2.5%, 1%\n",
    "\n",
    "    If the returned statistic is larger than these critical values then for the corresponding significance level, the null hypothesis that the data come from the chosen distribution can be rejected. The returned statistic is referred to as ‘A2’ in the references.\n",
    "    \n",
    "Tools for analyzing residuals:\n",
    "\n",
    "    For the basic analysis of residuals you will use the usual descriptive tools and scatterplots (plotting both fitted values and residuals, as well as the dependent and independent variables you have included in your model..\n",
    "\n",
    "        A histogram, dot-plot or stem-and-leaf plot lets you examine residuals: Standard regression assumes that residuals should be normally distributed. Study the shape of the distribution, watch for outliers and other unusual features.\n",
    "        A Q-Q Plot to assess normality of the residuals.\n",
    "        Plot the residuals against the dependent variable to zoom on the distances from the regression line. The picture you see should not show any particular pattern (random cloud). Look for outliers, groups, systematic features etc. to assess the fit in detail.\n",
    "        Plot the residuals against each independent variables to find out, whether a pattern is clearly related to one of the independents.\n",
    "        Plot the residuals against other variables to find out, whether a structure appearing in the residuals might be explained by another variable (a variable that you might want to include into a more complex model.\n",
    "        etc etc.\n",
    "        \n",
    "Leverage:\n",
    "    \n",
    "    In statistics and in particular in regression analysis, leverage is a measure of how far away the independent variable values of an observation are from those of the other observations. Leverage measures how far away the data point is from the mean value. In general 1/n ≤ hi ≤ 1. Where there are k independent variables in the model, the mean value for leverage is (k+1)/n. A rule of thumb (Steven's) is that values 3 times this mean value are considered large.\n",
    "        \n",
    "        \n",
    "Cook's Distance:\n",
    "\n",
    "    A general rule of thumb is that observations with a Cook's D of more than 3 times the mean, μ, is a possible outlier. An alternative interpretation is to investigate any point over 4/n, where n is the number of observations. Other authors suggest that any “large” Di should be investigated.Values of Cook’s distance of 1 or greater are generally viewed as high."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
